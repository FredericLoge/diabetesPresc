{
    "collab_server" : "",
    "contents" : "# R packages, crucial\nlibrary(arules) # discretize\nlibrary(nnet) # multinomial\n\n# R viz packages \nlibrary(ggplot2)\nlibrary(shiny)\nlibrary(shinydashboard)\n# library(dplyr)\n\n# source R files - the order is indicative of their use within this .R file\nsource('descriptive_app.R') # A shiny app and some ggplot, basic stat analysis of the data\nsource('read_and_parse_data.R') # Prepare the Diabetes data, collected within file 'datasets/'\nsource('construct_qlearning_ready_data.R') # Prepare the data for a RL format (S, A, R, S, A)\nsource('construct_dag_from_data.R') # Construct a Directed Acyclic Graph (dag) based on precedent data (which includes samplers)\nsource('simulators.R') # Data simulators built upon the dag\nsource('policies.R') # Implementing classic policies : Greedy, e-Greedy, UCB  \nsource('qlearning_and_variants.R') # QLearning algorithm and its variants (Batch) (Speedy) Q-Learning ; ideally would include Zap Q-Learning\nsource('reward.R') # Defining an appropriate reward function\nsource('evaluate_policy.R') # Policy evaluation approach based on simulators\nsource('viz_policy.R') # Visualize proposed policies\nsource('dynamic_prog.R')\n\n# extract function names\nget_function_names_from_R_file <- function(fn = 'descriptive_app.R'){\n  # read lines from R file\n  re <- readLines(con = fn)  \n  # identify lines with text '<- function('\n  re <- re[grep(pattern = '<- function(', x = re, fixed = TRUE)]\n  # extract function name\n  re <- sub(pattern = '<- function\\\\(.*', replacement = '', x = re)\n  return(re)\n}\nget_function_names_from_R_file('construct_qlearning_ready_data.R')\n\n# convention : global variables will have their names ending with '_'\n# list of global variables throughout file:\n#   tmp_; tm_;  dag_; levels_daytime_;  levels_action_; cuts_;  state_discretization_grid_ \n\n# load dataset\ntmp_ <- read_and_parse_diabetes_data()\n\n# GOTO descriptive_app.R for descriptive analytics\n# GOTO \"source('descriptive_analysis.R')\" for nice plots \n\n# construct dataset appropriate for Q-Learning\ntm_ <- construct_qlearning_ready_data()\n\n# checking on action variable\ntable(tm_$action_0)\n\n# removing rare action instances\ntm_ <- tm_[tm_$action_0 %in% names(which(table(tm_$action_0) > 100)),]\ntm_$action_0 <- factor(x = tm_$action_0, levels = sort(unique(tm_$action_0)))\n\n# keep\nlevels_daytime_ <- levels(tm_$daytime_0)\nlevels_action_ <- levels(tm_$action_0)\nnb_actions_ <- length(levels_action_)\n\n# creating state grid for glucose in order to build state representation\n## cuts_ <- discretize(x = tm_$glucose_0, method = 'cluster', categories = 4, onlycuts = TRUE)\ncuts_ <- c(0, 50, 80, 120, 200, max(tm_$glucose_0)*1.01)\nstate_discretization_grid_ <- expand.grid(\n  'daytime_0' = levels_daytime_, \n  'glucose_0_discretized' = levels(discretize(x = tm_$glucose_0, method = 'fixed', categories = cuts_)))\nnb_states_ <- nrow(state_discretization_grid_)\n\n# identify state ids\nget_state <- function(daytime_0, glucose_0){\n  glucose_0_discretized <- discretize(x = glucose_0, method = \"fixed\", categories = cuts_)\n  to_match_1 <- paste0(daytime_0, glucose_0_discretized)\n  to_match_2 <- apply(state_discretization_grid_, 1, paste0, collapse = '')\n  to_match_1 <- as.numeric(factor(x = to_match_1, levels = to_match_2))\n  return(to_match_1)\n}\nlevels_glucose_ <- unique(state_discretization_grid_$glucose_0_discretized)\nget_state_2 <- function(daytime_0, glucose_0){\n  glucose_0_index <- which(glucose_0 < cuts_)[1] - 1\n  index <- which(state_discretization_grid_$daytime_0 == daytime_0 &\n              state_discretization_grid_$glucose_0_discretized == levels_glucose_[glucose_0_index])\n  return(index)\n}\n\n#' @title Compute MDP elements from global dataset tm_, based on pre-specified Kernels\ncompute_kernelized_world_model <- function(tm_samp = tm_, index_reward_on_next_state = TRUE, use_kernel_for_rew = FALSE){\n\n  # state x action x state transition kernel\n  ktme <- expand.grid('s0' = 1:nb_states_, 'a0' = 1:nb_actions_)\n  temppp <- array(0, dim = c(nrow(ktme), nb_states_))\n  for(i in 1:nrow(ktme)){\n    \n    k1 <- k2 <- k3 <- rep(NA, nrow(tm_samp))\n    \n    # evaluate first kernel\n    state_ref <- state_discretization_grid_[ktme$s0[i],]\n    ind <- as.numeric(state_ref$glucose_0_discretized)\n    mu <- (cuts_[ind] + cuts_[ind+1])/2\n    ratio <- (10 * (mu - tm_samp$glucose_0) / (mu))^2\n    k1 <- exp( - ratio) * (tm_$daytime_0 == tm_)\n    \n    # evaluate second kernel\n    temp <- levels_action_[ktme$a0[i]]\n    if(TRUE){\n      k2 <- 1*(tm_samp$action_0 == temp)\n    }else{\n      temp <- as.numeric(strsplit(x = temp, split = '')[[1]])\n      wei <- rep(1, 3)\n      wei <- wei / sum(wei)\n      k2 <- wei[1] * (tm_samp$action_nph_0 == temp[1]) + wei[2] * (tm_samp$action_reg_0 == temp[2]) + wei[3] * (tm_samp$action_ult_0 == temp[3])\n    }\n    \n    cum_sum_k3 <- 0\n    for(s1_index in 1:nb_states_){\n      \n      # evaluate third kernel\n      state1_ref <- state_discretization_grid_[s1_index,]\n      ind <- as.numeric(state1_ref$glucose_0_discretized)\n      mu <- (cuts_[ind] + cuts_[ind+1])/2\n      ratio <- (10 * (mu - tm_samp$glucose_1) / (mu))^2\n      k3 <- exp( - ratio)\n      k3 <- k3 * ((as.numeric(tm_samp$daytime_0) - as.numeric(tm_samp$daytime_1)) %in% c(+1,-1))\n      \n      # store kernel value\n      temppp[i,s1_index] <- sum(k1*k2*k3) / sum(k1*k2)\n      cum_sum_k3 <- cum_sum_k3 + sum(k3)\n    }\n    \n    temppp[i,] <- temppp[i,]/sum(temppp[i,])\n    \n  }\n  ktme <- cbind(ktme, 's1_prob' = temppp)\n  \n  # adjacency matrix: putting aside never observed state x action pairs\n  adj <- array(data = 1, dim = c(nb_states_, nb_actions_))\n  index_not_ok <- rowSums(is.nan(as.matrix(ktme))) > 0\n  index_not_ok <- ktme[index_not_ok, c(1,2)]\n  for(i in 1:nrow(index_not_ok)){\n    adj[index_not_ok[i,1], index_not_ok[i,2]] <- 0\n  }\n  \n  # expected reward for given (state, action) pair -> could be kernelized as well, but a priori not necessary\n  rew <- expand.grid('s0' = 1:nb_states_, 'a0' = 1:nb_actions_, 's1' = 1:nb_states_)\n  rew$r0 <- NA\n  for(i in 1:nb_states_){\n    if(use_kernel_for_rew){\n      j <- as.numeric(state_discretization_grid_$glucose_0_discretized[i])\n      mu <- (cuts_[j] + cuts_[j+1])/2\n      ratio <- (10 * (mu - tm_samp$glucose_1) / (mu))^2\n      k4 <- exp( - ratio)\n      tj <- sum(k4 * reward(tm_samp$glucose_1)) / sum(k4)\n    }else{\n      j <- as.numeric(state_discretization_grid_$glucose_0_discretized[i])\n      condi <- (tm_samp$glucose_0 >= cuts_[j] & tm_samp$glucose_0 <= cuts_[j+1])\n      tj <- mean(reward(tm_samp$glucose_0[condi]))\n    }\n    if(index_reward_on_next_state == TRUE){\n      ind <- which(rew$s1 == i)\n    }else{\n      ind <- which(rew$s0 == i)\n    }\n    rew$r0[ind] <- tj\n  }\n  \n  # return elements\n  l <- list('tme' = ktme, 'adj' = adj, 'rew' = rew)\n  return(l)\n  \n}\n\n#' @title Compute MDP elements from global dataset tm_, based on pre-specified Kernels\ncompute_kernelized_world_model <- function(tm_samp = tm_, index_reward_on_next_state = TRUE, use_kernel_for_rew = FALSE){\n  \n  # compute discretized versions\n  state_tm <- get_state(daytime_0 = tm_samp$daytime_0, glucose_0 = tm_samp$glucose_0)\n  state_tm_tp1 <- c(state_tm[-1], 1)\n  \n  # state x action x state transition kernel\n  ktme <- expand.grid('s0' = 1:nb_states_, 'a0' = 1:nb_actions_)\n  temppp <- array(0, dim = c(nrow(ktme), nb_states_))\n  \n  #\n  for(i in 1:nrow(ktme)){\n    s0_index <- ktme$s0[i]\n    a0_index <- ktme$a0[i]\n    for(j in 1:nb_states_){\n      s1_index <- j\n      Tt <- state_discretization_grid_$daytime_0[s0_index]\n      Ttp1 <- state_discretization_grid_$daytime_0[s1_index]\n      Ot <- state_discretization_grid_$glucose_0_discretized[s0_index]\n      Otp1 <- state_discretization_grid_$glucose_0_discretized[s1_index]\n      At <- levels_action_[a0_index]\n      PROBA_1 <- sum(tm_samp$daytime_0 == Tt & tm_samp$daytime_1 == Ttp1) / sum(tm_samp$daytime_0 == Tt)\n      PROBA_2 <- sum(state_discretization_grid_$glucose_0_discretized[state_tm] == Ot & \n                       state_discretization_grid_$glucose_0_discretized[state_tm_tp1] == Otp1 & \n                       tm_samp$action_0 == At) / sum(state_discretization_grid_$glucose_0_discretized[state_tm] == Ot & tm_samp$action_0 == At)\n      temppp[i,j] <- PROBA_1 * PROBA_2\n    }\n  }\n\n  ktme <- cbind(ktme, 's1_prob' = temppp)\n  \n  # adjacency matrix: putting aside never observed state x action pairs\n  adj <- array(data = 1, dim = c(nb_states_, nb_actions_))\n  index_not_ok <- rowSums(is.nan(as.matrix(ktme))) > 0\n  index_not_ok <- ktme[index_not_ok, c(1,2)]\n  for(i in 1:nrow(index_not_ok)){\n    adj[index_not_ok[i,1], index_not_ok[i,2]] <- 0\n  }\n  \n  # expected reward for given (state, action) pair -> could be kernelized as well, but a priori not necessary\n  rew <- expand.grid('s0' = 1:nb_states_, 'a0' = 1:nb_actions_, 's1' = 1:nb_states_)\n  rew$r0 <- NA\n  for(i in 1:nb_states_){\n    if(use_kernel_for_rew){\n      j <- as.numeric(state_discretization_grid_$glucose_0_discretized[i])\n      mu <- (cuts_[j] + cuts_[j+1])/2\n      ratio <- (10 * (mu - tm_samp$glucose_1) / (mu))^2\n      k4 <- exp( - ratio)\n      tj <- sum(k4 * reward(tm_samp$glucose_1)) / sum(k4)\n    }else{\n      j <- as.numeric(state_discretization_grid_$glucose_0_discretized[i])\n      condi <- (tm_samp$glucose_0 >= cuts_[j] & tm_samp$glucose_0 <= cuts_[j+1])\n      tj <- mean(reward(tm_samp$glucose_0[condi]))\n    }\n    if(index_reward_on_next_state == TRUE){\n      ind <- which(rew$s1 == i)\n    }else{\n      ind <- which(rew$s0 == i)\n    }\n    rew$r0[ind] <- tj\n  }\n  \n  # return elements\n  l <- list('tme' = ktme, 'adj' = adj, 'rew' = rew)\n  return(l)\n  \n}\n\n#' @title Compute MDP elements from global dataset tm_, crossing all modalities of state variables\ncompute_world_model <- function(tm_samp = tm_, index_reward_on_next_state = TRUE){\n\n  # state x action x state transition matrix\n  tme <- expand.grid('s0' = 1:nb_states_, 'a0' = 1:nb_actions_)\n  temp <- array(0, dim = c(nrow(tme), nb_states_))\n  a0 <- as.numeric(tm_samp$action_0)\n  s0 <- apply(tm_samp[,c(\"daytime_0\", \"glucose_0\")], 1, function(x) get_state(daytime_0 = x[1], glucose_0 = x[2]))\n  s1 <- get_state(daytime_0 = tm_samp$daytime_1, glucose_0 = tm_samp$glucose_1)\n  ni <- numeric(nb_states_)\n  for(i in 1:nrow(tme)){\n    cond <- ((s0 == tme$s0[i]) & (a0 == tme$a0[i]))\n    temp[i,] <- as.numeric(table(x = factor(x = s1[cond], levels = 1:nb_states_)))\n    ni[i] <- sum(temp[i,])\n    temp[i,] <- temp[i,] / sum(temp[i,])\n  }\n  tme <- cbind(tme, 's1_prob' = temp)\n\n  # adjacency matrix: putting aside never observed state x action pairs\n  adj <- array(data = 1, dim = c(nb_states_, nb_actions_))\n  index_not_ok <- which(ni < 1)\n  index_not_ok <- tme[index_not_ok, c(1,2)]\n  for(i in 1:nrow(index_not_ok)){\n    adj[index_not_ok[i,1], index_not_ok[i,2]] <- 0\n  }\n  \n  # expected reward for given (state, action) pair\n  rew <- expand.grid('s0' = 1:nb_states_, 'a0' = 1:nb_actions_, 's1' = 1:nb_states_)\n  rew$r0 <- NA\n  for(i in 1:nb_states_){\n    j <- as.numeric(state_discretization_grid_$glucose_0_discretized[i])\n    condi <- (tm_samp$glucose_0 >= cuts_[j] & tm_samp$glucose_0 <= cuts_[j+1])\n    tj <- mean(reward(tm_samp$glucose_0[condi]))\n    ## tj <- mean(reward(runif(n = 1e4, min = cuts_[j], max = cuts_[j+1])))\n    if(index_reward_on_next_state == TRUE){\n      ind <- which(rew$s1 == i)\n    }else{\n      ind <- which(rew$s0 == i)\n    }\n    rew$r0[ind] <- tj\n  }\n  \n  # return such values\n  return(list('tme' = tme, 'adj' = adj, 'rew' = rew))\n  \n}\n\n# estimate world model\nwm0 <- compute_world_model(index_reward_on_next_state = TRUE)\nwm1 <- compute_kernelized_world_model(use_kernel_for_rew = FALSE)\n\n#\nimage(x = t(as.matrix(wm0$tme[,-(1:2)]))>0)\nsum(t(as.matrix(wm0$tme[,-(1:2)]))>0, na.rm=TRUE)\nsum(is.na((as.matrix(wm0$tme[,-(1:2)]))))\ndim(wm0$tme[,-(1:2)])\n# look at differences in transition matrices\nimage(x = t(as.matrix(wm0$tme[,-(1:2)])))\nimage(x = (as.matrix(wm1$tme[,-(1:2)])))\nnew_tme <- rbind.data.frame(wm0$tme[order(wm0$tme$s0, wm0$tme$a0),], wm1$tme)\nnew_tme$model <- rep(c('classic', 'kernel'), each = nrow(wm0$tme))\nnew_tme$id <- paste0('(', new_tme$s0, ', ', new_tme$a0, ')')\nnew_tme$id <- factor(new_tme$id, levels = new_tme$id[1:nrow(wm0$tme)])\nnew_tme <- data.table::melt(data = new_tme[,-(1:2)], id.vars = c('id', 'model'), value.name = 'value')\nnew_tme$variable <- factor(sub('s1_prob.', '', new_tme$variable, fixed = TRUE), levels = 1:nb_states_)\nm <- 20\nggplot(data = new_tme) +\n  geom_tile(mapping = aes(x = variable, y = id, fill = value)) +\n  facet_grid(~model) +\n  xlab(expression(S[t+1])) +\n  ylab(expression(paste('(', S[t], ', ', A[t], ')'))) +\n  ggtitle('Transition matrix estimates based on classic estimates and their Kernel versions\\n') +\n  labs(fill = 'Transition probability   ') +\n  theme(\n    text = element_text(size = 10),\n    legend.position = 'bottom',\n    legend.title = element_text(size = 20),\n    title = element_text(size = 25, margin = margin(m,m,m,m), vjust = 0.5), \n    plot.margin = margin(m, m, m, m),\n    axis.title.x = element_text(size = 20, margin = margin(m,m,m,m)),\n    axis.title.y = element_text(size = 20, margin = margin(m,m,m,m), hjust = 1/2, angle = 0)\n  )\n\nestim <- data.frame(\n  state_discretization_grid_[,2],\n  round(wm0$rew$r0[wm0$rew$s0 == 1 & wm0$rew$a0 == 1],3),\n  round(wm1$rew$r0[wm1$rew$s0 == 1 & wm1$rew$a0 == 1],3))\nunique(estim)\ncat(paste0(apply(unique(estim), 1 , paste0, collapse = '  &  '), collapse = '\\\\\\\\ \\n'))\n#\nsoftmax <- function(x, beta){\n  prop.table(exp(x*beta), 1)\n}\n\n# myopic optimization\ngamma_ <- 0\ndp0_wm0 <- dynamic_programming(tme = wm0$tme, rew = wm0$rew, adj = wm0$adj)\ndp0_wm1 <- dynamic_programming(tme = wm1$tme, rew = wm1$rew, adj = wm1$adj)\napply(dp0_wm0[[2]], 1, greedy)\napply(dp0_wm1[[2]], 1, greedy)\nplot_my_heatmap(mat = softmax(x = dp0_wm0[[1]], beta = 10), r = rn, c = NULL, te = dp0_wm0[[1]])\nplot_my_heatmap(mat = softmax(x = dp0_wm1[[1]], beta = 10), r = rn, c = NULL, te = dp0_wm1[[1]])\n\n# in this myopic criterion, it seems that action 3 is brought into the light thanks to\n# kernelized estimates, which detect its good 1-step performance\n\n# mid-term optimization\ngamma_ <- 0.80\ndp1_wm0 <- dynamic_programming(tme = wm0$tme, rew = wm0$rew, adj = wm0$adj)\ndp1_wm1 <- dynamic_programming(tme = wm1$tme, rew = wm1$rew, adj = wm1$adj)\napply(dp1_wm0[[2]], 1, greedy)\napply(dp1_wm1[[2]], 1, greedy)\nplot_my_heatmap(mat = dp1_wm0[[1]], r = NULL, c = NULL, te = dp1_wm0[[1]])\nplot_my_heatmap(mat = dp1_wm1[[1]], r = NULL, c = NULL, te = dp1_wm1[[1]])\nplot_my_heatmap(mat = softmax(x = dp1_wm0[[1]], beta = 10), r = NULL, c = NULL, te = dp1_wm0[[1]])\nplot_my_heatmap(mat = softmax(x = dp1_wm1[[1]], beta = 10), r = NULL, c = NULL, te = dp1_wm1[[1]])\n\nnewdf <- rbind(dp1_wm0[[1]], dp1_wm1[[1]])\n# normalize ?\nnewdf <- softmax(newdf, beta = 5)\n#\nnewdf <- as.data.frame(newdf)\ncolnames(newdf) <- paste0('a', 1:5)\nnewdf$model <- rep(c('classic', 'kernel'), each = nb_states_)\nnewdf$state <- rep(paste0('s', 1:nb_states_), times = 2)\nnewdf$state <- factor(newdf$state, levels = paste0('s', 1:nb_states_))\nnewdf <- data.table::melt(data = newdf, id.vars = c('state', 'model'))\nnewdf$value[newdf$value == 0] <- NA\nnewdf$rescaled_value <- newdf$value\nfor(l in unique(newdf$model)){\n  cond <- (newdf$model == l)\n  min_value <- min(newdf$value[cond], na.rm = TRUE)\n  max_value <- max(newdf$value[cond], na.rm = TRUE)\n  newdf$rescaled_value[cond] <- (newdf$rescaled_value[cond] - min_value) / (max_value - min_value)\n}\nm <- 20\nggplot(data = newdf) +\n  geom_tile(mapping = aes(x = variable, y = state, fill = value)) +\n  facet_grid(~model) +\n  xlab(expression(A[t])) +\n  ylab(expression(S[t])) +\n  ggtitle('State-Action function estimates - normalized\\n') +\n  labs(fill = expression(paste(exp(beta*hat(Q)[sa]), \" /\", sum(exp(beta*hat(Q)[si]), i==1, 20) ))) +\n  scale_fill_continuous(low = 'white', high = 'darkblue', na.value = 'black') +\n  theme(\n    text = element_text(size = 10),\n    legend.position = 'bottom',\n    legend.title = element_text(size = 20),\n    title = element_text(size = 25, margin = margin(m,m,m,m), vjust = 0.5), \n    plot.margin = margin(m, m, m, m),\n    axis.title.x = element_text(size = 20, margin = margin(m,m,m,m)),\n    axis.title.y = element_text(size = 20, margin = margin(m,m,m,m), hjust = 1/2, angle = 0)\n  )\n\n# comparatives of recommendations given\nsummary(c(abs(softmax(x = dp1_wm0[[1]], beta = 10) - softmax(x = dp0_wm0[[1]], beta = 10))))\nsummary(c(abs(softmax(x = dp1_wm1[[1]], beta = 10) - softmax(x = dp0_wm1[[1]], beta = 10))))\nsummary(c(abs(softmax(x = dp0_wm0[[1]], beta = 10) - softmax(x = dp0_wm1[[1]], beta = 10))))\nsummary(c(abs(softmax(x = dp1_wm0[[1]], beta = 10) - softmax(x = dp1_wm1[[1]], beta = 10))))\n\n#\nm3 <- dynamic_programming_2()\ncbind(m3[[2]], dp1_wm0[[1]])\ncbind(m3[[1]], dp1_wm0[[2]])\ntable(apply(m3[[1]], 1, greedy), apply(dp1_wm0[[2]], 1, greedy))\n\n#\nlibrary(dplyr)\nwm0$tme %>% filter(a0 == 1 & s0 == 1)\nwm0$rew %>% filter(a0 == 1 & s0 == 1)\nmdp_learnt$transition %>% filter(a0 == 1 & s0 == 1)\nmdp_learnt$reward %>% filter(a0 == 1 & s0 == 1)\n\n# transition_matrix_estimate_ <- ktme\n# state_action_adjacency_ <- array(data = 1, dim = c(nb_states_, nb_actions_))\n# index_not_ok <- rowSums(is.nan(as.matrix(ktme))) > 0\n# index_not_ok <- ktme[index_not_ok, c(1,2)]\n# for(i in 1:nrow(index_not_ok)){\n#   state_action_adjacency_[index_not_ok[i,1], index_not_ok[i,2]] <- 0\n# }\ndp1 <- dynamic_programming()\nhead(dp0[[1]])\nhead(dp1[[1]])\nimage(t(prop.table(dp0[[1]], 1)))\nimage(t(prop.table(dp1[[1]], 1)))\nmatt <- dp1[[1]]\nfor(i in 1:nrow(matt)){\n  matt[index_not_ok[i,1], index_not_ok[i,2]] <- NA\n}\nplot_my_heatmap(mat = matt, r = rn, c = NULL, te = dp1[[1]])\nplot_my_heatmap(mat = prop.table(exp(10*dp1[[1]]),1), r = rn, c = NULL, te = dp1[[1]])\nplot_my_heatmap(mat = dp0[[1]], r = rn, c = NULL, te = dp0[[1]])\nplot_my_heatmap(mat = prop.table(exp(10*dp0[[1]]),1), r = rn, c = NULL, te = dp0[[1]])\napply(dp0[[2]], 1, function(x) which(x==1))\napply(dp1[[2]], 1, function(x) which(x==1))\n\n# approach 1. construct non-parametric model estimates and apply Dynammic Programming\n\n# construct Directed Acyclic Graph (dag) from tm_ data \ndag_ <- construct_dag_from_data()\n\n# TODO add dag representation\n\n\n## TODO : add a simulator from category of GLUCOSE to continuous GLUCOSE.\n\n## TODO : constrain the action space conditionally to the state information\n## gros probleme : couples etats-action rares\n## idee: contraindre l'espace d'etat pour apprendre nos controles\n\n# for graphical representations\nrn <- with(state_discretization_grid_, paste0('D: ', daytime_0, '; G: ', glucose_0_discretized))\ncn <- NULL ## levels_action_\n\n# model 0: empirical policy\nm0 <- get_empirical_policy()\nplot_my_heatmap(mat = prop.table(m0, 1), r = NULL, c = NULL, te = m0)\n\nnewdf <- prop.table(m0, 1)\ncolnames(newdf) <- paste0('a', 1:5)\nrownames(newdf) <- paste0('s', 1:20)\nnewdf <- as.data.frame(newdf)\n\nm <- 20\nggplot(data = newdf) +\n  geom_tile(mapping = aes(x = Var2, y = Var1, fill = Freq)) +\n  xlab(expression(A[t])) +\n  ylab(expression(S[t])) +\n  ggtitle('State-Action function counts from real data \\n') +\n  labs(fill = expression(paste(N[sa], \" /\", sum(N[si], i==1, 20), '  '))) +\n  scale_fill_continuous(low = 'white', high = 'darkblue', na.value = 'black') +\n  theme(\n    text = element_text(size = 10),\n    legend.position = 'bottom',\n    legend.title = element_text(size = 20),\n    title = element_text(size = 25, margin = margin(m,m,m,m), vjust = 0.5), \n    plot.margin = margin(m, m, m, m),\n    axis.title.x = element_text(size = 20, margin = margin(m,m,m,m)),\n    axis.title.y = element_text(size = 20, margin = margin(m,m,m,m), hjust = 1/2, angle = 0)\n  )\n\n# model 1: speedy batch Q-Learning, sampling policy: uniform\nm1 <- batch_qlearning(\n  my_simulate_batch = function(N, reco, prior_init){\n    simulate_batch(N = N, reco = reco, prior_init = prior_init)\n  },\n  prior_over_states = NULL,\n  gamma = 0.8, batch_size = 10, \n  qsa_max_abs_diff_stopping = 1e-03, max_nb_runs = 20000,\n  do_speedy_qlearning = TRUE, alpha_k_indexed_on_s_a = TRUE, omega = 1,\n  sampling_policy = c('uniform', 'e_greedy_10pct', 'greedy', 'ucb'))\n\n# average reward behavior throughout epochs\nplot.ts(m1$avg_reward, main = 'Average return per iteration')\nplot_my_heatmap(mat = m1$qsa, r = rn, c = NULL, te = m1$qsa)\nsoftmax_qsa <- prop.table(exp(10*m1$qsa), 1)\nplot_my_heatmap(mat = softmax_qsa, r = rn, c = NULL, te = m1$qsa)\n\n# model 2: batch Q-Learning, sampling policy: uniform\nm2 <- batch_qlearning(\n  my_simulate_batch = function(N, reco, prior_init){\n    simulate_batch(N = N, reco = reco, prior_init = prior_init)\n  },\n  prior_over_states = NULL,\n  gamma = 0.9, batch_size = 10, \n  qsa_max_abs_diff_stopping = 1e-03, max_nb_runs = 2000,\n  do_speedy_qlearning = FALSE, alpha_k_indexed_on_s_a = TRUE, omega = 0.6,\n  sampling_policy = c('uniform', 'e_greedy_10pct', 'greedy', 'ucb'))\n\n# average reward behavior throughout epochs\nplot.ts(m2$avg_reward, main = 'Average return per iteration')\nplot_my_heatmap(mat = m2$qsa, r = rn, c = NULL, te = m2$qsa)\nsoftmax_qsa <- prop.table(exp(10*m2$qsa), 1)\nplot_my_heatmap(mat = softmax_qsa, r = rn, c = NULL, te = m2$qsa)\n\n### Comparing results from m1 and m2\n### both results are coherent, so that's a first good news !\n### it means Speedy just speeds it up, did not get us to \n### a completely different solution\n\n### Let's look at other sampling policies\n\n# model 3: batch Q-Learning, sampling policy: greedy\nm3 <- batch_qlearning(\n  my_simulate_batch = function(N, reco, prior_init){\n    simulate_batch(N = N, reco = reco, prior_init = prior_init)\n  }, initial_qsa = m1$qsa,\n  prior_over_states = NULL,\n  gamma = 0.8, batch_size = 10, \n  qsa_max_abs_diff_stopping = 1e-04, max_nb_runs = 20000,\n  do_speedy_qlearning = TRUE, alpha_k_indexed_on_s_a = TRUE, omega = 0.6,\n  sampling_policy = 'greedy')\nm3uw <- batch_qlearning(\n  my_simulate_batch = function(N, reco, prior_init){\n    simulate_batch(N = N, reco = reco, prior_init = prior_init)\n  }, initial_qsa = m1$qsa,\n  prior_over_states = NULL,\n  gamma = 0.9, batch_size = 10, \n  qsa_max_abs_diff_stopping = 1e-04, max_nb_runs = 20000,\n  do_speedy_qlearning = TRUE, alpha_k_indexed_on_s_a = TRUE, omega = 0.6,\n  sampling_policy = 'greedy')\n\n# m3_mem <- list()\nm3_mem[[length(m3_mem)+1]] <- m3$qsa\n## m3_mem[[length(m3_mem)]] <- NULL\n\ni <- i + 1\nplot_my_heatmap(mat = m4_mem[[i]], r = rn, c = cn, te = '')\n\n\nplot.ts(m1$avg_reward[-length(m1$avg_reward)], ylim = c(0,6), xlim = c(0, 5000))\nlines(m2$avg_reward[-length(m2$avg_reward)], col = 'darkblue')\nlines(m3$avg_reward[-length(m3$avg_reward)], col = 'lightblue')\nlines(m3uw$avg_reward[-length(m3uw$avg_reward)], col = 'blue')\n\nnb_experiments <- 100\nlength_traj <- 80\ngamma <- 0.9\nm3_reco <- apply(m3$qsa, 1, greedy)\nm3_eval <- evaluate_reco(reco = m3_reco, nb_experiments = nb_experiments, length_traj = length_traj, gamma = gamma)\nm3uw_reco <- apply(m3uw$qsa, 1, greedy)\nm3uw_eval <- evaluate_reco(reco = m3uw_reco, nb_experiments = nb_experiments, length_traj = length_traj, gamma = gamma)\nsummary(m3_eval)\nsummary(m3uw_eval)\n\nm4_reco <- apply(m4$qsa, 1, e_greedy, e = 0.1)\nm4_eval <- evaluate_reco(reco = m4_reco, nb_experiments = nb_experiments, length_traj = length_traj, gamma = gamma)\nm5_reco <- sapply(1:nb_states_, function(s) ucb(q = m5$qsa[s,], n = m5$nsa[s,]))\nm5_eval <- evaluate_reco(reco = m5_reco, nb_experiments = nb_experiments, length_traj = length_traj, gamma = gamma)\n\n# model 4: batch Q-Learning, sampling policy: greedy\nm4 <- batch_qlearning(\n  my_simulate_batch = function(N, reco, prior_init){\n    simulate_batch(N = N, reco = reco, prior_init = prior_init)\n  }, initial_qsa = m4$qsa,\n  prior_over_states = NULL,\n  gamma = 0.8, batch_size = 30, \n  qsa_max_abs_diff_stopping = 1e-04, max_nb_runs = 20000,\n  do_speedy_qlearning = TRUE, alpha_k_indexed_on_s_a = TRUE, omega = 0.6,\n  sampling_policy = 'e_greedy_10pct')\n\n# m4_mem <- list()\nm4_mem[[length(m4_mem)+1]] <- m4$qsa\n## m4_mem[[length(m4_mem)]] <- NULL\n\ni = 0\ni <- i + 1\nplot_my_heatmap(mat = softmax(m4_mem[[i]], beta=5), r = NULL, c = NULL, te = '')\n\n# model 5: batch Q-Learning, sampling policy: greedy\nm5 <- batch_qlearning(\n  my_simulate_batch = function(N, reco, prior_init){\n    simulate_batch(N = N, reco = reco, prior_init = prior_init)\n  },\n  prior_over_states = NULL,\n  gamma = 0.9, batch_size = 10, \n  qsa_max_abs_diff_stopping = 1e-06, max_nb_runs = 20000,\n  do_speedy_qlearning = TRUE, alpha_k_indexed_on_s_a = TRUE, omega = 0.7,\n  sampling_policy = 'ucb')\n\n# model 6: batch Q-Learning, e-greedy: 20\\%\nm6 <- batch_qlearning(\n  my_simulate_batch = function(N, reco, prior_init){\n    simulate_batch(N = N, reco = reco, prior_init = prior_init)\n  },\n  prior_over_states = NULL,\n  gamma = 0.9, batch_size = 10, \n  qsa_max_abs_diff_stopping = 1e-7, max_nb_runs = 2000,\n  do_speedy_qlearning = TRUE, alpha_k_indexed_on_s_a = TRUE, omega = 0.7,\n  sampling_policy = 'e_greedy_20pct')\nm7 <- batch_qlearning(\n  my_simulate_batch = function(N, reco, prior_init){\n    simulate_batch(N = N, reco = reco, prior_init = prior_init)\n  }, \n  initial_qsa = m6$qsa, \n  prior_over_states = NULL,\n  gamma = 0.9, batch_size = 100, \n  qsa_max_abs_diff_stopping = 1e-10, max_nb_runs = 2000,\n  do_speedy_qlearning = TRUE, alpha_k_indexed_on_s_a = TRUE, omega = 0.7,\n  sampling_policy = 'greedy')\n\n\n# average reward behavior throughout epochs\nplot.ts(m3$avg_reward, main = 'Average return per iteration')\nplot.ts(m4$avg_reward, col = 'darkblue')\nplot.ts(m5$avg_reward, col = 'blue')\nplot.ts(m6$avg_reward)\nplot.ts(m7$avg_reward, col = 'red')\n\n# very different recommendations\nsoftmat_coef <- 1\nplot_my_heatmap(mat = prop.table(exp(softmat_coef*m1$qsa), 1), r = rn, c = NULL, te = m1$qsa)\nplot_my_heatmap(mat = prop.table(exp(softmat_coef*m2$qsa), 1), r = rn, c = NULL, te = m2$qsa)\nplot_my_heatmap(mat = prop.table(exp(softmat_coef*m3$qsa), 1), r = rn, c = NULL, te = m3$qsa)\nplot_my_heatmap(mat = prop.table(exp(softmat_coef*m4$qsa), 1), r = rn, c = NULL, te = m4$qsa)\nplot_my_heatmap(mat = prop.table(exp(softmat_coef*m6$qsa), 1), r = rn, c = NULL, te = m6$qsa)\nplot_my_heatmap(mat = prop.table(exp(softmat_coef*m5$qsa), 1), r = rn, c = NULL, te = m5$qsa)\n\nplot_my_heatmap(mat = m3$qsa, r = rn, c = NULL, te = m3$qsa)\nplot_my_heatmap(mat = m4$qsa, r = rn, c = NULL, te = m4$qsa)\n\nplot_my_heatmap(mat = m6$qsa, r = rn, c = NULL, te = m6$qsa)\nplot_my_heatmap(mat = m7$qsa, r = rn, c = NULL, te = m7$qsa)\n\n\n# # evaluate policy followed on real data\n# get_empirical_policy <- function(){\n#   # recover states throughout dataset\n#   s0 <- get_state(daytime_0 = tm_$daytime_0, glucose_0 = tm_$glucose_0)\n#   s0 <- factor(s0, levels = 1:nb_states_)\n#   # cross states and actions from available dataset\n#   a0 <- factor(tm_$action_0, levels = levels_action_)\n#   # recover states throughout dataset\n#   s1 <- c(s0[-1],NA)\n#   # return Q matrix\n#   prop.table(table(s1[a0 == '000'], s0[a0 == '000']))\n#   return(nsa)\n# }\n\n#\nnb_experiments <- 100\nlength_traj <- 80\ngamma <- 0.9\nm3_reco <- apply(m3$qsa, 1, greedy)\nm3_eval <- evaluate_reco(reco = m3_reco, nb_experiments = nb_experiments, length_traj = length_traj, gamma = gamma)\nm4_reco <- apply(m4$qsa, 1, e_greedy, e = 0.1)\nm4_eval <- evaluate_reco(reco = m4_reco, nb_experiments = nb_experiments, length_traj = length_traj, gamma = gamma)\nm5_reco <- sapply(1:nb_states_, function(s) ucb(q = m5$qsa[s,], n = m5$nsa[s,]))\nm5_eval <- evaluate_reco(reco = m5_reco, nb_experiments = nb_experiments, length_traj = length_traj, gamma = gamma)\n\n#\ncbind(m3_reco, m4_reco, m5_reco)\n\n#\nmodel_eval <- data.frame('model' = -1, 'value' = c(m3_eval, m4_eval, m5_eval))\nmodel_eval$model <- rep(x = 3:5, times = nb_experiments)\nggplot(data = model_eval) +\n  geom_violin(mapping = aes(x = factor(model), y = value))\nggplot(data = model_eval) +\n  geom_histogram(mapping = aes(col = factor(model), x = value))\nby(model_eval$value, model_eval$model, summary)\n\n#\nsim <- simulate_batch(N = 1000, reco = base::sample(x = 1:nb_actions_, replace = TRUE, size = nb_states_))\nsim$reward <- reward(sim$glucose_0)\nplot.ts(cumsum(0.9^{1:1000 - 1} * sim$reward))\n\n# SYNCHRONOUS Q-LEARNING\n#\n#\n\n# continuous sampler of glucose value, conditionally to the fact it belongs in some range\nclustered_ref <- arules::discretize(x = tm_$glucose_0, method = 'fixed', categories = cuts)\nfrom_discrete_to_conti <- function(glucose_0_discretized){\n  base::sample(x = tm_$glucose_0[clustered_ref == glucose_0_discretized], size = 1)\n}\n\n\n## TODO NEXT:\n## > METTRE UNE ACTION REALISTE / DONE!\n## > CHOISIR PARAMETRES (ALPHA, GAMMA) ASSURANT LA CONVERGENCE / DONE!\n## > ADD FUNCTIONAL REPRESENTATIONS\n## > SPEEDY Q-LEARNING / DOES NOT WORK BETTER THAN Q-LEARNING ... ISN'T THAT WEIRD?\n## > CREATE SIMULATOR WITH DESIGNATED STARTING STATE TO PROPERLY IMPLEMENT Q-LEARNING\n## > EVALUATION OFFLINE DE POLICY SAUVEGARDEES\n## > DEEP Q-NETWORK\n## > PROPOSER D'AUTRES DAG PLUS COMPLEXES ? PEUT-ETRE PAS BESOIN SI LE MODELE INITIAL EST VRAI.",
    "created" : 1548868936485.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1009123589",
    "id" : "4D2A1998",
    "lastKnownWriteTime" : 1549203068,
    "last_content_update" : 1549258867516,
    "path" : "~/Documents/kdd_2019_diabetes/kdd_diabetes/main.R",
    "project_path" : "main.R",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}