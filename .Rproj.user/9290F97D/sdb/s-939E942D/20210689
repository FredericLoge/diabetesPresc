{
    "collab_server" : "",
    "contents" : "## TODO: \n## - ADD TIMER ESTIMATE\n## - MASK FOR CONSTRAINED ACTION\n\n#' @title Batch Q-Learning\n#' @param my_simulate_batch simulator\n#' @param nb_states\n#' @param nb_actions\n#' @param prior_over_states\n#' @param gamma = 0.9, \n#' @param batch_size = 100, \n#' @param qsa_max_abs_diff_stopping = 1e-03, \n#' @param max_nb_runs = 2000,\n#' @param  do_speedy_qlearning = TRUE, \n#' @param alpha_k_indexed_on_s_a = TRUE, \n#' @param omega = 1,\n#' @param sampling_policy c('uniform', 'e_greedy_10pct', 'greedy', 'ucb')\nbatch_qlearning <- function(\n  my_simulate_batch = function(N, reco, prior_init){ NULL },\n  initial_qsa = array(data = 0, dim = c(nb_states_, nb_actions_)),\n  prior_over_states,\n  gamma = 0.9, batch_size = 100, \n  qsa_max_abs_diff_stopping = 1e-03, max_nb_runs = 2000,\n  do_speedy_qlearning = TRUE, alpha_k_indexed_on_s_a = TRUE, omega = 1,\n  sampling_policy = c('uniform', 'e_greedy_10pct', 'greedy', 'ucb')){\n  \n  # keep first element \n  sampling_policy <- sampling_policy[1]\n  \n  # initialize Q(s,a)\n  qsa_old <- qsa <- nsa <- initial_qsa\n  \n  # store average rewards obtained\n  avg_reward <- numeric(length = max_nb_runs)\n  \n  ## core algorithm\n  k <- 1 # counter\n  convergence_not_attained <- TRUE\n  while(convergence_not_attained){\n    \n    # store old Q(s,a) matrices\n    qsa_very_old <- qsa_old\n    qsa_old <- qsa\n    \n    # choose next actions via sub-optimal policy\n    ## current_reco <- get_reco_from_qtable(...)\n    if(sampling_policy == 'uniform'){\n      current_reco = base::sample(x = 1:nb_actions_, replace = TRUE, size = nb_states_)\n    }else if(sampling_policy == 'greedy'){\n      current_reco = apply(qsa, 1, greedy)\n    }else if(sampling_policy == 'e_greedy_10pct'){\n      current_reco = apply(qsa, 1, e_greedy, e = 0.1)\n    }else if(sampling_policy == 'e_greedy_20pct'){\n      current_reco = apply(qsa, 1, e_greedy, e = 0.2)\n    }else if(sampling_policy == 'ucb'){\n      current_reco = sapply(1:nb_states_, function(s) ucb(q = qsa[s,], n = nsa[s,]))\n    }else{\n      stop('Sorry, declared policy was not recognized.')\n    }\n    \n    # store average reward under selected policy, weighted by prior over states\n    if(!is.null(prior_over_states)){\n      avg_reward[k] <- sum(prior_states * sapply(1:nb_states_, function(s) qsa[s,current_reco[s]])) / sum(prior_states)\n    }else{\n      avg_reward[k] <- mean(sapply(1:nb_states_, function(s) qsa[s,current_reco[s]]))\n    }\n    \n    # simulate new batch of data\n    simu <- my_simulate_batch(N = batch_size, reco = current_reco, prior_init = prior_over_states)\n    \n    # get state and rewards\n    simu$state <- NA\n    for(i in 1:nrow(simu)){\n      simu$state[i] <- get_state(daytime_0 = simu$daytime_0[i], glucose_0 = simu$glucose_0[i])\n    }\n    simu$reward <- c(reward(simu$glucose_0[-1]), NA)\n    \n    # set learning rate, \\alpha_k\n    alpha_k <- 1/(k^omega)\n    \n    # for every transition:\n    for(i in 1:(nrow(simu)-1)){\n      \n      # get (state, action, reward, state)\n      s0 <- simu$state[i]\n      a0 <- as.numeric(simu$action_0[i])\n      r0 <- simu$reward[i]\n      s1 <- simu$state[i+1]\n      \n      # add counts to nsa\n      nsa[s0, a0] <- nsa[s0, a0] + 1 \n      \n      #\n      if(alpha_k_indexed_on_s_a){\n        alpha_k <- 1 / (nsa[s0,a0]+1)\n      }\n      \n      # update rules\n      if(do_speedy_qlearning){\n        # update rule for Speedy Q-Learning\n        update_0 <- r0 + gamma * max(qsa_very_old[s1,])\n        update_1 <- r0 + gamma * max(qsa_old[s1,])\n        qsa[s0,a0] <- qsa_old[s0,a0] + alpha_k * (update_0 - qsa_old[s0,a0]) + (1 - alpha_k) * (update_1 - update_0)\n      }else{\n        # update rule for regular Q-Learning\n        qsa[s0,a0] <- qsa_old[s0,a0] + alpha_k * (r0 + gamma * max(qsa_old[s1,]) - qsa_old[s0,a0])\n      }\n      \n    }\n    \n    # step counter\n    k <- k + 1\n    \n    # stopping conditions check\n    convergence_not_attained <- (max(abs(qsa - qsa_old)) > qsa_max_abs_diff_stopping)\n    if(k > max_nb_runs) convergence_not_attained <- FALSE\n    \n  }\n  \n  # return convergence status, algorithm controls, qsa table\n  l <- list('convergence' = !convergence_not_attained, \n            'algo_ctrl' = list(prior_over_states,\n                               gamma, batch_size,\n                               qsa_max_abs_diff_stopping, max_nb_runs,\n                               do_speedy_qlearning, alpha_k_indexed_on_s_a, omega,\n                               sampling_policy),\n            'qsa' = qsa,\n            'nsa' = nsa,\n            'avg_reward' = avg_reward[1:k])\n  return(l)\n  \n}\n\n",
    "created" : 1549124232276.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "38|34|117|2|\n",
    "hash" : "2714416406",
    "id" : "20210689",
    "lastKnownWriteTime" : 1549189697,
    "last_content_update" : 1549190644183,
    "path" : "~/Documents/kdd_2019_diabetes/kdd_diabetes/qlearning_and_variants.R",
    "project_path" : "qlearning_and_variants.R",
    "properties" : {
    },
    "relative_order" : 11,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}