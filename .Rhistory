cbind(m3[[2]], dp1_wm0[[1]])
cbind(m3[[1]], dp1_wm0[[2]])
adj
compute_world_model <- function(tm_samp = tm_, index_reward_on_next_state = TRUE){
# state x action x state transition matrix
tme <- expand.grid('s0' = 1:nb_states_, 'a0' = 1:nb_actions_)
temp <- array(0, dim = c(nrow(tme), nb_states_))
a0 <- as.numeric(tm_samp$action_0)
s0 <- get_state(daytime_0 = tm_samp$daytime_0, glucose_0 = tm_samp$glucose_0)
s1 <- get_state(daytime_0 = tm_samp$daytime_1, glucose_0 = tm_samp$glucose_1)
ni <- numeric(nb_states_)
for(i in 1:nrow(tme)){
cond <- ((s0 == tme$s0[i]) & (a0 == tme$a0[i]))
temp[i,] <- as.numeric(table(x = factor(x = s1[cond], levels = 1:nb_states_)))
ni[i] <- sum(temp[i,])
temp[i,] <- temp[i,] / sum(temp[i,])
}
tme <- cbind(tme, 's1_prob' = temp)
# adjacency matrix: putting aside never observed state x action pairs
adj <- array(data = 1, dim = c(nb_states_, nb_actions_))
index_not_ok <- which(ni < 1)
index_not_ok <- tme[index_not_ok, c(1,2)]
for(i in 1:nrow(index_not_ok)){
adj[index_not_ok[i,1], index_not_ok[i,2]] <- 0
}
# expected reward for given (state, action) pair
rew <- expand.grid('s0' = 1:nb_states_, 'a0' = 1:nb_actions_, 's1' = 1:nb_states_)
rew$r0 <- NA
for(i in 1:nb_states_){
j <- as.numeric(state_discretization_grid_$glucose_0_discretized[i])
tj <- mean(reward(runif(n = 1e4, min = cuts_[j], max = cuts_[j+1])))
if(index_reward_on_next_state == TRUE){
ind <- which(rew$s1 == i)
}else{
ind <- which(rew$s0 == i)
}
rew$r0[ind] <- tj
}
# return such values
return(list('tme' = tme, 'adj' = adj, 'rew' = rew))
}
wm0 <- compute_world_model()
dp0_wm0 <- dynamic_programming(tme = wm0$tme, rew = wm0$rew, adj = wm0$adj)
gamma_
dp1_wm0[[1]]
gamma_
wm0$adj
dp1_wm0 <- dynamic_programming(tme = wm0$tme, rew = wm0$rew, adj = wm0$adj)
dp1_wm0
cbind(m3[[2]], dp1_wm0[[1]])
dynamic_programming <- function(tme, rew, adj){
# solve optimization problem
pi_mat <- init_pi(adj)
qsa_mat <- compute_qsa_based_on_pi(pi_mat, tme , rew, adj)
has_not_converged <- TRUE
count <- 0
while(has_not_converged){
## action_weights <- colSums(pi_mat) * as.numeric(rowSums(adj) %*% adj)
old_qsa_mat <- qsa_mat
## temp <- compute_optimalpi_based_on_qsa(qsa_mat)
temp <- apply(qsa_mat, 1, greedy)
temp <- t(sapply(temp, function(x){ v <- rep(0,nb_actions_) ; v[x] <- 1 ; v}))
pi_mat <- temp
qsa_mat <- compute_qsa_based_on_pi(pi_mat, tme, rew, adj)
has_not_converged <- ( max(qsa_mat - old_qsa_mat) > 1e-04 )
count <- count + 1
}
return(list(qsa_mat, pi_mat))
}
dp1_wm0 <- dynamic_programming(tme = wm0$tme, rew = wm0$rew, adj = wm0$adj)
dp1_wm0[[1]]
cbind(m3[[2]], dp1_wm0[[1]])
compute_world_model <- function(tm_samp = tm_, index_reward_on_next_state = TRUE){
# state x action x state transition matrix
tme <- expand.grid('s0' = 1:nb_states_, 'a0' = 1:nb_actions_)
temp <- array(0, dim = c(nrow(tme), nb_states_))
a0 <- as.numeric(tm_samp$action_0)
s0 <- get_state(daytime_0 = tm_samp$daytime_0, glucose_0 = tm_samp$glucose_0)
s1 <- get_state(daytime_0 = tm_samp$daytime_1, glucose_0 = tm_samp$glucose_1)
ni <- numeric(nb_states_)
for(i in 1:nrow(tme)){
cond <- ((s0 == tme$s0[i]) & (a0 == tme$a0[i]))
temp[i,] <- as.numeric(table(x = factor(x = s1[cond], levels = 1:nb_states_)))
ni[i] <- sum(temp[i,])
temp[i,] <- temp[i,] / sum(temp[i,])
}
tme <- cbind(tme, 's1_prob' = temp)
# adjacency matrix: putting aside never observed state x action pairs
adj <- array(data = 1, dim = c(nb_states_, nb_actions_))
index_not_ok <- which(ni < 1)
index_not_ok <- tme[index_not_ok, c(1,2)]
for(i in 1:nrow(index_not_ok)){
adj[index_not_ok[i,1], index_not_ok[i,2]] <- 0
}
# expected reward for given (state, action) pair
rew <- expand.grid('s0' = 1:nb_states_, 'a0' = 1:nb_actions_, 's1' = 1:nb_states_)
rew$r0 <- NA
for(i in 1:nb_states_){
j <- as.numeric(state_discretization_grid_$glucose_0_discretized[i])
condi <- (tm_samp$glucose_0 >= cuts_[j] & tm_samp$glucose_0 <= cuts_[j+1])
tj <- mean(reward(tm_samp$glucose_0[condi]))
## tj <- mean(reward(runif(n = 1e4, min = cuts_[j], max = cuts_[j+1])))
if(index_reward_on_next_state == TRUE){
ind <- which(rew$s1 == i)
}else{
ind <- which(rew$s0 == i)
}
rew$r0[ind] <- tj
}
# return such values
return(list('tme' = tme, 'adj' = adj, 'rew' = rew))
}
wm0 <- compute_world_model()
gamma_ <- 0
dp0_wm0 <- dynamic_programming(tme = wm0$tme, rew = wm0$rew, adj = wm0$adj)
dp0_wm0
gamma_ <- 0.50
dp1_wm0 <- dynamic_programming(tme = wm0$tme, rew = wm0$rew, adj = wm0$adj)
apply(dp1_wm0[[2]], 1, greedy)
cbind(m3[[2]], dp1_wm0[[1]])
apply(dp1_wm0[[2]], 1, greedy)
table(apply(m3[[1]], 1, greedy), apply(dp1_wm0[[2]], 1, greedy))
m3[[1]]
dp1_wm0[[2]]
mdp_learnt$reward
mdp_learnt$reward[mdp_learnt$reward$s0==1 & mdp_learnt$reward$a0==1,]
mdp_learnt <- learn_mdp_model(reward_indep_next_state = FALSE, state = sta0, new_state = sta1, action = act0, reward = rew0, initial_model = NULL)
mdp_learnt
mdp_learnt$reward[mdp_learnt$reward$s0==1 & mdp_learnt$reward$a0==1,]
ns <-  max(mdp_learnt$reward$s0)
na <- max(mdp_learnt$reward$a0)
softmax <- FALSE
dynamic_programming_2 <- function(){
sta0 <- get_state(daytime_0 = tm_$daytime_0, glucose_0 = tm_$glucose_0)
sta1 <- get_state(daytime_0 = tm_$daytime_1, glucose_0 = tm_$glucose_1)
act0 <- tm_$action_0
rew0 <- reward(tm_$glucose_1)
mdp_learnt <- learn_mdp_model(reward_indep_next_state = FALSE, state = sta0, new_state = sta1, action = act0, reward = rew0, initial_model = NULL)
### m_test <- td_lookup(state = sta0, action = act0, new_state = sta1, reward = rew0)
ns <-  max(mdp_learnt$reward$s0)
na <- max(mdp_learnt$reward$a0)
# policy
softmax <- FALSE
epsilon_par <- 0.0 # 0.05
tau <- 1
# vector of state value function v(s)
vs <- rep(0, ns)
# probability matrix p(a | s)
psa <- matrix(data = rep(1/na, ns*na), nrow = ns, ncol = na)
psa[1,] <- psa[ns,] <- 0
# transition probability P(s' | s, a)
tsa <- mdp_learnt$transition
# reward function
reward_vec <- mdp_learnt$reward
# is state terminal ?
isTerminal <- rep(FALSE, ns)
# gamma parameter
gamma <- 0.5
#
j <- 1
not_stopping <- TRUE
while(j < 1000 & not_stopping){
# save previous state value functions
pvs <- vs
# iterate over states
for(i in 1:ns){
if(!isTerminal[i]){
# update state values for each probable output
vs[i] <- 0
for(k in 1:na){
new_state <- tsa[tsa$s0 == i & tsa$a0 == k,]
vs[i] <- vs[i] + psa[i,k]*(reward_vec$mean_value[reward_vec$s0 == i & reward_vec$a0 == k] + gamma*(new_state$prob %*% pvs[new_state$s1]))
}
}
}
# update policy
for(i in 1:ns){
for(k in 1:na){
new_state <- tsa[tsa$s0 == i & tsa$a0 == k,]
psa[i,k] <- reward_vec$mean_value[reward_vec$s0 == i & reward_vec$a0 == k] + gamma * (new_state$prob %*% pvs[new_state$s1])
}
if(softmax){
# first possibility : softmax
psa[i,] <- exp(psa[i,] / tau)
psa[i,] <- psa[i,] / sum(psa[i,])
}else{
# second : \epsilon-greedy policy
max_index <- which(psa[i,] == max(psa[i,]))
psa[i,] <- 0
if(runif(n = 1) < epsilon_par){
max_index <- sample(x = 1:na, size = 1)
}else{
if(length(max_index)>1) max_index <- sample(x = max_index, size = 1)
}
psa[i, max_index] <- 1
}
}
# print value functions
## cat('\n') cat(round(vs, 1))
not_stopping <- (max(abs(pvs - vs)) > 1e-01)
# counter update
j <- j + 1
}
qsa <- array(data = 0, dim = c(ns, na))
for(i in 1:ns){
for(k in 1:na){
new_state <- tsa[tsa$s0 == i & tsa$a0 == k,]
qsa[i,k] <- reward_vec$mean_value[reward_vec$s0 == i & reward_vec$a0 == k] + gamma * (new_state$prob %*% pvs[new_state$s1])
}
}
return(list(psa, qsa))
}
m3 <- dynamic_programming_2()
dynamic_programming_2 <- function(){
sta0 <- get_state(daytime_0 = tm_$daytime_0, glucose_0 = tm_$glucose_0)
sta1 <- get_state(daytime_0 = tm_$daytime_1, glucose_0 = tm_$glucose_1)
act0 <- tm_$action_0
rew0 <- reward(tm_$glucose_1)
mdp_learnt <- learn_mdp_model(reward_indep_next_state = TRUE, state = sta0, new_state = sta1, action = act0, reward = rew0, initial_model = NULL)
### m_test <- td_lookup(state = sta0, action = act0, new_state = sta1, reward = rew0)
ns <-  max(mdp_learnt$reward$s0)
na <- max(mdp_learnt$reward$a0)
# policy
softmax <- FALSE
epsilon_par <- 0.0 # 0.05
tau <- 1
# vector of state value function v(s)
vs <- rep(0, ns)
# probability matrix p(a | s)
psa <- matrix(data = rep(1/na, ns*na), nrow = ns, ncol = na)
psa[1,] <- psa[ns,] <- 0
# transition probability P(s' | s, a)
tsa <- mdp_learnt$transition
# reward function
reward_vec <- mdp_learnt$reward
# is state terminal ?
isTerminal <- rep(FALSE, ns)
# gamma parameter
gamma <- 0.5
#
j <- 1
not_stopping <- TRUE
while(j < 1000 & not_stopping){
# save previous state value functions
pvs <- vs
# iterate over states
for(i in 1:ns){
if(!isTerminal[i]){
# update state values for each probable output
vs[i] <- 0
for(k in 1:na){
new_state <- tsa[tsa$s0 == i & tsa$a0 == k,]
vs[i] <- vs[i] + psa[i,k]*(reward_vec$mean_value[reward_vec$s0 == i & reward_vec$a0 == k] + gamma*(new_state$prob %*% pvs[new_state$s1]))
}
}
}
# update policy
for(i in 1:ns){
for(k in 1:na){
new_state <- tsa[tsa$s0 == i & tsa$a0 == k,]
psa[i,k] <- reward_vec$mean_value[reward_vec$s0 == i & reward_vec$a0 == k] + gamma * (new_state$prob %*% pvs[new_state$s1])
}
if(softmax){
# first possibility : softmax
psa[i,] <- exp(psa[i,] / tau)
psa[i,] <- psa[i,] / sum(psa[i,])
}else{
# second : \epsilon-greedy policy
max_index <- which(psa[i,] == max(psa[i,]))
psa[i,] <- 0
if(runif(n = 1) < epsilon_par){
max_index <- sample(x = 1:na, size = 1)
}else{
if(length(max_index)>1) max_index <- sample(x = max_index, size = 1)
}
psa[i, max_index] <- 1
}
}
# print value functions
## cat('\n') cat(round(vs, 1))
not_stopping <- (max(abs(pvs - vs)) > 1e-01)
# counter update
j <- j + 1
}
qsa <- array(data = 0, dim = c(ns, na))
for(i in 1:ns){
for(k in 1:na){
new_state <- tsa[tsa$s0 == i & tsa$a0 == k,]
qsa[i,k] <- reward_vec$mean_value[reward_vec$s0 == i & reward_vec$a0 == k] + gamma * (new_state$prob %*% pvs[new_state$s1])
}
}
return(list(psa, qsa))
}
dynamic_programming_2 <- function(){
sta0 <- get_state(daytime_0 = tm_$daytime_0, glucose_0 = tm_$glucose_0)
sta1 <- get_state(daytime_0 = tm_$daytime_1, glucose_0 = tm_$glucose_1)
act0 <- tm_$action_0
rew0 <- reward(tm_$glucose_0)
mdp_learnt <- learn_mdp_model(reward_indep_next_state = TRUE, state = sta0, new_state = sta1, action = act0, reward = rew0, initial_model = NULL)
### m_test <- td_lookup(state = sta0, action = act0, new_state = sta1, reward = rew0)
ns <-  max(mdp_learnt$reward$s0)
na <- max(mdp_learnt$reward$a0)
# policy
softmax <- FALSE
epsilon_par <- 0.0 # 0.05
tau <- 1
# vector of state value function v(s)
vs <- rep(0, ns)
# probability matrix p(a | s)
psa <- matrix(data = rep(1/na, ns*na), nrow = ns, ncol = na)
psa[1,] <- psa[ns,] <- 0
# transition probability P(s' | s, a)
tsa <- mdp_learnt$transition
# reward function
reward_vec <- mdp_learnt$reward
# is state terminal ?
isTerminal <- rep(FALSE, ns)
# gamma parameter
gamma <- 0.5
#
j <- 1
not_stopping <- TRUE
while(j < 1000 & not_stopping){
# save previous state value functions
pvs <- vs
# iterate over states
for(i in 1:ns){
if(!isTerminal[i]){
# update state values for each probable output
vs[i] <- 0
for(k in 1:na){
new_state <- tsa[tsa$s0 == i & tsa$a0 == k,]
vs[i] <- vs[i] + psa[i,k]*(reward_vec$mean_value[reward_vec$s0 == i & reward_vec$a0 == k] + gamma*(new_state$prob %*% pvs[new_state$s1]))
}
}
}
# update policy
for(i in 1:ns){
for(k in 1:na){
new_state <- tsa[tsa$s0 == i & tsa$a0 == k,]
psa[i,k] <- reward_vec$mean_value[reward_vec$s0 == i & reward_vec$a0 == k] + gamma * (new_state$prob %*% pvs[new_state$s1])
}
if(softmax){
# first possibility : softmax
psa[i,] <- exp(psa[i,] / tau)
psa[i,] <- psa[i,] / sum(psa[i,])
}else{
# second : \epsilon-greedy policy
max_index <- which(psa[i,] == max(psa[i,]))
psa[i,] <- 0
if(runif(n = 1) < epsilon_par){
max_index <- sample(x = 1:na, size = 1)
}else{
if(length(max_index)>1) max_index <- sample(x = max_index, size = 1)
}
psa[i, max_index] <- 1
}
}
# print value functions
## cat('\n') cat(round(vs, 1))
not_stopping <- (max(abs(pvs - vs)) > 1e-01)
# counter update
j <- j + 1
}
qsa <- array(data = 0, dim = c(ns, na))
for(i in 1:ns){
for(k in 1:na){
new_state <- tsa[tsa$s0 == i & tsa$a0 == k,]
qsa[i,k] <- reward_vec$mean_value[reward_vec$s0 == i & reward_vec$a0 == k] + gamma * (new_state$prob %*% pvs[new_state$s1])
}
}
return(list(psa, qsa))
}
compute_world_model <- function(tm_samp = tm_, index_reward_on_next_state = TRUE){
# state x action x state transition matrix
tme <- expand.grid('s0' = 1:nb_states_, 'a0' = 1:nb_actions_)
temp <- array(0, dim = c(nrow(tme), nb_states_))
a0 <- as.numeric(tm_samp$action_0)
s0 <- get_state(daytime_0 = tm_samp$daytime_0, glucose_0 = tm_samp$glucose_0)
s1 <- get_state(daytime_0 = tm_samp$daytime_1, glucose_0 = tm_samp$glucose_1)
ni <- numeric(nb_states_)
for(i in 1:nrow(tme)){
cond <- ((s0 == tme$s0[i]) & (a0 == tme$a0[i]))
temp[i,] <- as.numeric(table(x = factor(x = s1[cond], levels = 1:nb_states_)))
ni[i] <- sum(temp[i,])
temp[i,] <- temp[i,] / sum(temp[i,])
}
tme <- cbind(tme, 's1_prob' = temp)
# adjacency matrix: putting aside never observed state x action pairs
adj <- array(data = 1, dim = c(nb_states_, nb_actions_))
index_not_ok <- which(ni < 1)
index_not_ok <- tme[index_not_ok, c(1,2)]
for(i in 1:nrow(index_not_ok)){
adj[index_not_ok[i,1], index_not_ok[i,2]] <- 0
}
# expected reward for given (state, action) pair
rew <- expand.grid('s0' = 1:nb_states_, 'a0' = 1:nb_actions_, 's1' = 1:nb_states_)
rew$r0 <- NA
for(i in 1:nb_states_){
j <- as.numeric(state_discretization_grid_$glucose_0_discretized[i])
condi <- (tm_samp$glucose_0 >= cuts_[j] & tm_samp$glucose_0 <= cuts_[j+1])
tj <- mean(reward(tm_samp$glucose_0[condi]))
## tj <- mean(reward(runif(n = 1e4, min = cuts_[j], max = cuts_[j+1])))
if(index_reward_on_next_state == TRUE){
ind <- which(rew$s1 == i)
}else{
ind <- which(rew$s0 == i)
}
rew$r0[ind] <- tj
}
# return such values
return(list('tme' = tme, 'adj' = adj, 'rew' = rew))
}
wm0 <- compute_world_model(index_reward_on_next_state = FALSE)
gamma_ <- 0.50
dp1_wm0 <- dynamic_programming(tme = wm0$tme, rew = wm0$rew, adj = wm0$adj)
dp1_wm0
m3 <- dynamic_programming_2()
cbind(m3[[2]], dp1_wm0[[1]])
table(apply(m3[[1]], 1, greedy), apply(dp1_wm0[[2]], 1, greedy))
dynamic_programming_2
dynamic_programming_2 <- function(){
sta0 <- get_state(daytime_0 = tm_$daytime_0, glucose_0 = tm_$glucose_0)
sta1 <- get_state(daytime_0 = tm_$daytime_1, glucose_0 = tm_$glucose_1)
act0 <- tm_$action_0
rew0 <- reward(tm_$glucose_0)
mdp_learnt <- learn_mdp_model(reward_indep_next_state = TRUE, state = sta0, new_state = sta1, action = act0, reward = rew0, initial_model = NULL)
### m_test <- td_lookup(state = sta0, action = act0, new_state = sta1, reward = rew0)
ns <-  max(mdp_learnt$reward$s0)
na <- max(mdp_learnt$reward$a0)
# policy
softmax <- FALSE
epsilon_par <- 0.0 # 0.05
tau <- 1
# vector of state value function v(s)
vs <- rep(0, ns)
# probability matrix p(a | s)
psa <- matrix(data = rep(1/na, ns*na), nrow = ns, ncol = na)
psa[1,] <- psa[ns,] <- 0
# transition probability P(s' | s, a)
tsa <- mdp_learnt$transition
# reward function
reward_vec <- mdp_learnt$reward
# is state terminal ?
isTerminal <- rep(FALSE, ns)
# gamma parameter
gamma <- 0.5
#
j <- 1
not_stopping <- TRUE
while(j < 20000 & not_stopping){
# save previous state value functions
pvs <- vs
# iterate over states
for(i in 1:ns){
if(!isTerminal[i]){
# update state values for each probable output
vs[i] <- 0
for(k in 1:na){
new_state <- tsa[tsa$s0 == i & tsa$a0 == k,]
vs[i] <- vs[i] + psa[i,k]*(reward_vec$mean_value[reward_vec$s0 == i & reward_vec$a0 == k] + gamma*(new_state$prob %*% pvs[new_state$s1]))
}
}
}
# update policy
for(i in 1:ns){
for(k in 1:na){
new_state <- tsa[tsa$s0 == i & tsa$a0 == k,]
psa[i,k] <- reward_vec$mean_value[reward_vec$s0 == i & reward_vec$a0 == k] + gamma * (new_state$prob %*% pvs[new_state$s1])
}
if(softmax){
# first possibility : softmax
psa[i,] <- exp(psa[i,] / tau)
psa[i,] <- psa[i,] / sum(psa[i,])
}else{
# second : \epsilon-greedy policy
max_index <- which(psa[i,] == max(psa[i,]))
psa[i,] <- 0
if(runif(n = 1) < epsilon_par){
max_index <- sample(x = 1:na, size = 1)
}else{
if(length(max_index)>1) max_index <- sample(x = max_index, size = 1)
}
psa[i, max_index] <- 1
}
}
# print value functions
## cat('\n') cat(round(vs, 1))
not_stopping <- (max(abs(pvs - vs)) > 1e-03)
# counter update
j <- j + 1
}
qsa <- array(data = 0, dim = c(ns, na))
for(i in 1:ns){
for(k in 1:na){
new_state <- tsa[tsa$s0 == i & tsa$a0 == k,]
qsa[i,k] <- reward_vec$mean_value[reward_vec$s0 == i & reward_vec$a0 == k] + gamma * (new_state$prob %*% pvs[new_state$s1])
}
}
return(list(psa, qsa))
}
gamma_ <- 0.50
dp1_wm0 <- dynamic_programming(tme = wm0$tme, rew = wm0$rew, adj = wm0$adj)
m3 <- dynamic_programming_2()
cbind(m3[[2]], dp1_wm0[[1]])
table(apply(m3[[1]], 1, greedy), apply(dp1_wm0[[2]], 1, greedy))
wm0$rew
wm0$rew[wm0$rew$s0==1 & wm0$rew$a0==1,]
mdp_learnt$reward[mdp_learnt$reward]
mdp_learnt$reward[mdp_learnt$reward$s0==1 & mdp_learnt$reward$a0==1, ]
mdp_learnt <- learn_mdp_model(reward_indep_next_state = TRUE, state = sta0, new_state = sta1, action = act0, reward = rew0, initial_model = NULL)
mdp_learnt$reward[mdp_learnt$reward$s0==1 & mdp_learnt$reward$a0==1, ]
